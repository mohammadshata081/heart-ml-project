{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "## Heart Disease Prediction Dataset\n",
        "\n",
        "This notebook provides comprehensive exploratory data analysis of the heart disease dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add parent directory to path to import custom modules\n",
        "sys.path.append('..')\n",
        "from src import data_preprocessing as dp\n",
        "from src import eda\n",
        "from config import DATASET_PATH, PROCESSED_DATA_PATH, OUTPUT_FIGURES_DIR\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data\n",
        "df = dp.load_data()\n",
        "print(f'Dataset shape: {df.shape}')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate data\n",
        "validation = dp.validate_data(df)\n",
        "print('Validation Results:')\n",
        "print(f'Shape: {validation[\"shape\"]}')\n",
        "print(f'Missing values: {sum(validation[\"missing_values\"].values())}')\n",
        "print(f'Duplicates: {validation[\"duplicates\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "df_processed = dp.preprocess_data(df)\n",
        "print(f'Processed dataset shape: {df_processed.shape}')\n",
        "\n",
        "# Save processed data\n",
        "dp.save_processed_data(df_processed)\n",
        "df_processed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get dataset overview\n",
        "overview = eda.get_dataset_overview(df_processed)\n",
        "print('Dataset Overview:')\n",
        "for key, value in overview.items():\n",
        "    print(f'{key}: {value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "summary_stats = dp.get_summary_statistics(df_processed)\n",
        "summary_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Missing Values Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze missing values\n",
        "missing_analysis = eda.analyze_missing_values(df_processed)\n",
        "print(f'Total missing values: {missing_analysis[\"total_missing\"]}')\n",
        "print(f'Percentage missing: {missing_analysis[\"percentage_missing\"]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Target Variable Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze target variable\n",
        "target_analysis = eda.analyze_target_variable(df_processed)\n",
        "print('Target Variable Analysis:')\n",
        "print(f'Value counts: {target_analysis[\"value_counts\"]}')\n",
        "print(f'Percentages: {target_analysis[\"percentages\"]}')\n",
        "print(f'Is balanced: {target_analysis[\"is_balanced\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize target distribution\n",
        "target_fig = eda.visualize_target_distribution(df_processed)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Histograms (Minimum 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create histograms for numeric columns\n",
        "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'target' in numeric_cols:\n",
        "    numeric_cols.remove('target')\n",
        "\n",
        "# Select at least 3 columns for histograms\n",
        "histogram_cols = numeric_cols[:max(3, len(numeric_cols))]\n",
        "print(f'Creating histograms for: {histogram_cols}')\n",
        "\n",
        "hist_fig = eda.create_histograms(df_processed, columns=histogram_cols)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Boxplots (Minimum 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create boxplots for numeric columns\n",
        "boxplot_cols = numeric_cols[:max(3, len(numeric_cols))]\n",
        "print(f'Creating boxplots for: {boxplot_cols}')\n",
        "\n",
        "box_fig = eda.create_boxplots(df_processed, columns=boxplot_cols)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create correlation heatmap\n",
        "corr_fig = eda.create_correlation_heatmap(df_processed)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Key Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate EDA insights\n",
        "insights = eda.generate_eda_insights(df_processed)\n",
        "print('Key Insights:')\n",
        "print('=' * 60)\n",
        "for section, content in insights.items():\n",
        "    print(f'\\n{section.upper()}:')\n",
        "    print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has completed a comprehensive exploratory data analysis including:\n",
        "\n",
        "1. ✅ Data loading and validation\n",
        "2. ✅ Data preprocessing and saving processed data\n",
        "3. ✅ Dataset overview and summary statistics\n",
        "4. ✅ Missing values analysis\n",
        "5. ✅ Target variable distribution analysis\n",
        "6. ✅ Histograms (minimum 3)\n",
        "7. ✅ Boxplots (minimum 3)\n",
        "8. ✅ Correlation heatmap\n",
        "9. ✅ Key insights generation\n",
        "\n",
        "The processed dataset has been saved to `data/processed/heart_disease_processed.csv` and can be used for model training."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
